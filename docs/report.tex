\documentclass[11pt,conference,letterpaper]{IEEEtran}
\usepackage{amssymb,amsmath}

\begin{document}

\title{Impromptica: Automatic Music Accompaniment Software}
\author{\IEEEauthorblockN{Patrick Shields, Chris Hudson, and Siddhant Sharma}}

\maketitle

\begin{abstract}
We present music accompaniment software called Impromptica, which generates accompaniment using feature recognition and probabilistic models. The idea of the accompaniment is to provide insights and exploration into existing musical ideas.
\end{abstract}

\section{Introduction}

For our CS capstone project at the University of Kansas, we are designing and implementing software which utilizes feature extraction, simple sound generation, and generative probabilistic models to render an input audio track with musical accompaniment. We call this software Impromptica. Impromptica uses python, scipy, and Vamp plugins for analysis and feature extraction, and handful of other tools for music generation, including Audiolab, Hydrogen drum kits, and soundfonts. The general goal of Impromptica is to provide a simple way to expand on and explore musical ideas --- like whistled tunes or short piano ostinatos --- via accompaniment.

\subsection{Background}

Generative music and feature extraction have a rich, though young, background. The term generative music, coined by Brian Eno, has come to describe the process of using software programs to create music using a myriad of techniques, such as generative grammars or procedural processes \cite{wooller2005framework}. 

While some music generation techniques are solely functional, with no apriori information, many music generation techniques depend on musical feature recognition via feature extraction, which is still an evolving field. Feature extraction includes obtaining information about the underlying metric structure of music, such as tempo, rhythm, key, and melody-related information, such as note onsets and note frequencies. The analyzation of pulse-code modulation, the standard form of digital audio, is used to accomplish this recognition, and novel algorithms are published frequently exploring different techniques with varying success. This information informs decisions throughout the accompaniment generation. 

Recent research has explored concepts such as k-NN regression to tempo recognition \cite{eronen2010music} and using spectral smoothness to analyze the frequency of polyphonic music \cite{klapuri2003multiple}. Some areas have yet to be heavily explored, such as musical analysis via machine learning.

Work has been done on automatically accompanying vocal tracks with chords \cite{simon2008mysong} using machine learning. \cite{morris2008exposing}

Feature extraction plays an important role in music generation, and forms the foundation of music accompaniment. Rather than accompany an input audio file directly, musical features are extracted from the file and used to inform decisions made throughout the course of accompaniment generation.

\subsection{Motivation}

Despite our limited background in music and music theory, we had an interest in exploring generative music using machines. Music and math are strongly connected. The intersection of computer science and music theory has produced valuable results, but many avenues are still unexplored. Investigating music theory through the lens of mathematics and computing yields opportunities to explore discrete frequencies of sound and how their complex interactions, together with rhythm, produce music. Probabilistic analysis of music offers a simple, elegant way to mathematically interpret music, model the components of a musical piece, and, we believe, generate musical accompaniment.

Compared to other methods of algorithmic music composition, many of which consist of a patchwork of hard-coded rules and heuristics, probabilistic music generation offers a higher-level approach. By learning parameters from a corpus of existing music, probabilistic music generation promises musical insights powered by Bayes' Rule:

{\small
  \[ P(\text{structure}|\text{surface}) = \frac{P(\text{surface}|\text{structure})P(\text{structure})}{P(\text{surface})} \]
}

% TODO Discuss more real-world applications, such as accompaniment of live performances.

\section{Methodology}

The project has feature extraction and accompaniment components. Our work so far has focused primarily on feature extraction.

\subsection{Software environment}

Impromptica is written in Python and currently has several dependencies, which are described in the repository README.

\subsection{Feature extraction}

Impromptica uses Sonic Annotator in tandem with Vamp plugins to extract RDF-formatted feature information.

The input audio file is processed through a variety of feature extraction modules which annotate the original audio track with information about musical features such as tempo and the location and duration of note onsets within the piece. Onset and note detection is assisted by the modal library. Impromptica also currently includes implementation of polyphonic key-finding algorithms based on those found in Music and Probability \cite{temperly2007mprob}, as well as meter recognition algorithms from \cite{klapuri2006analysis}, beat tracking algorithms from \cite{ellis2007beat}, and pitch detection algorithms from \cite{klapuri2003multiple}.

We implemented note, meter, and key extraction algorithms for Impromptica before discovering Sonic Annotator, a command-line interface to a body of feature extraction tools implemented as Vamp plugins (described more fully in \cite{cannam2010sonic}.) We are presently configuring Impromptica to delegate all feature recognition tasks to Sonic Annotator and associated Vamp plugins.

\subsection{Accompaniment generation}

We will utilize probabilistic generative models (see \cite{temperly2007mprob}, \cite{conklin2003music}, and \cite{mccormack1996grammar}) to generate the notes and rhythm of the accompaniment. We have made some progress on this front. Patrick implemented probabilistic note generation using data from Music and Probability, but we have not integrated it into the output yet. Where probabilistic generation is intractable, we will explore the use of other algorithmic music composition algorithms as heuristics to decrease the space of possibilities. We will note the extent to which this modifies the probability distribution of generated music.

Another part of the project will be to explore various featuresâ€™ amenability to probabilistic modelling. Beyond predicting the next note based on the previous note, we will try to apply probabilistic generative models to more complex phenomena such as chords, harmonies, rhythm and other patterns.

We also have to implement logic to decide which types of accompaniment to produce (e.g.\ percussive, melodic, harmonic, atmospheric) and when.

As time allows, we may compare probabilistic generative models with other methods of algorithmic music composition methods such as cellular automata, genetic algorithms, and constraint-based methods.

\subsection{Instrumentation}

Finally, we will render the accompaniment by creating our own virtual instruments or, where appropriate, plugging into existing virtual instruments or synthesizers. We have completed some of this work; Chris wrote utility methods for writing audio to files and using soundfonts for synthesized notes rather than digitally-generated waves.

\section{Deliverables}

\subsection{Feature detection algorithms}

The software will contain methods for detection of various features as provided in the project methodology. Third-party libraries may be used for assistance in some of the methods.

\subsection{Accompaniment generation logic}

When rendering accompaniment onto input audio, the software will select from various forms of accompaniment (as provided in the project methodology) and determine which regions of the piece are most appropriate for accompaniment.

\subsection{Command-line interface}

Through the command-line interface, a user may specify an input audio file for Impromptica to provide accompaniment for. Taking that file as input, Impromptica renders musical accompaniment and provides the user with the resulting audio file.

\subsection{Web interface}

The software will provide a web-based interface offering similar functionality. The web interface may also be used for debugging or data visualization, as time allows.

\subsection{Experimentation, data collection and visualization}

We will analyze the efficacy of probabilistic music generation by comparing the music it creates to that generated by alternative methods.  We will also compare the overall aesthetic of the generated music to that of conventional music.

\section{Discussion}

\subsection {Music theory}
As a precursor to any implementation, we had to spend some time understanding some fundamental music concepts and how music has been modeled in different systems throughout history. We supplemented this exploration through several books, such as \cite{temperly2007mprob}, \cite{krogerGeeksNerds}, and \cite{loy2006musimathics}. This provided several insights into the mathematical nature of music that we were able to map to a computational realm. For instance, the equal-tempered chromatic scale functions on a logarithmic scale - that is, a particular semitones's frequency can be modeled by the equation
{\small
	\[frequency = 261.63 * 2^{(\text{value} - 60) / 12.0)}\]
}
where 261.63 represents the frequency of middle C, 60 represents the semitone of middle C, and value represents the semitone we are solving for, relative to middle C.

Other insights included a exploration of timbre and soundfonts, permutation techniques like inversion and retrograde, interval sonorities, and several other musical constructs.

\subsection{Music ontology for feature extraction}

A large body of work is forming in the area of musical feature extraction. Much of this work is fueled by interests in music information retrieval, where algorithms run on music datasets to identify metadata for applications such as recommender systems. As the number of extracted features has grown, some focus has shifted towards the development of feature extraction frameworks and standards. In such systems, unrelated feature extraction tools may plug in to existing feature extraction systems with little or no additional configuration. The Vamp plugins system takes this approach, defining an API through which programmers may write feature extraction plugins. Tools such as Sonic Annotator provide a mechanism to apply specified Vamp plugins to specified audio files.

Prior work has examined how feature extraction can be represented through a modular, unifying ontology. \cite{raimond2008web}. Vamp plugins use (non-excusively) the music \cite{raimond2007music} and event \cite{raimond2007event} ontologies to represent extracted features. The SAWA web application \cite{fazekas2009reusable} uses many of the previously-mentioned tools to provide an online API for audio analysis.

% TODO Discuss pros and cons of the RDF approach.

Running multiple feature extraction tools on an audio file presents the opportunity to eliminate redudant transformations of the audio. For example, many feature extraction tools utilize information from the frequency domain, obtained by taking windowed Fast Fourier Transform at steps throughout the piece. Such a derived signal could be calculated once and reused by each feature extraction tool which needs it. YAAFE \cite{mathieu2010yaafe} is an example of a system which attempts to remove redundant steps in the plans it creates to extract specified features from input audio files.

\subsection{Machine learning for accompaniment}

The MySong project, which used machine learning to select chords for accompaniment of a vocal melody, exposed some model parameters to end users \cite{morris2008exposing}. To keep things simple, Impromptica does not take this approach, choosing instead to make educated guesses of appropriate parameter values given the data it already had.

% TODO Include examples of how Impromptica makes educated guesses of appropriate parameter values.

\subsection{Real-time accompaniment challenges}

Impromptica, in its current form, does not attempt to accompany music in real time. Real-time accompaniment is more difficult because future features are not known and must be predicted from past and present extracted features.

% TODO Expand this subsection.

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{report}

\end{document}
