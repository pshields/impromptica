\documentclass[11pt,conference,letterpaper]{IEEEtran}
\usepackage{amssymb,amsmath}

\begin{document}

\title{Impromptica: Automatic Music Accompaniment Software}
\author{\IEEEauthorblockN{Patrick Shields, Chris Hudson, and Siddhant Sharma}}

\maketitle

\begin{abstract}
We present music accompaniment software called Impromptica. Unlike traditional accompaniment software, which aligns a known musical score to the real-time performance of a soloist, Impromptica accompanies music for which the score is unknown and may not even exist, such as audio recordings of musical improvisation. Impromptica uses the existing Vamp plugin architecture to convert an input audio file into a digital representation of the musical features. It then uses a genetic algorithm to generate accompaniment and render it onto the original audio, producing an accompanied audio file.

Impromptica is written in Python licensed under an open-source license. Its source code is publibly available on GitHub at \emph{https://github.com/chris-hudson/impromptica/}.

 % TODO Decide which license to use.

\end{abstract}

\section{Introduction}

For our computer science capstone project at the University of Kansas, we designed and implemented music accompaniment software which brings together feature extraction algorithms, sound generation facilities, musical data structures, generative probabilistic models, and musical fitness functions to render an input audio track with musical accompaniment. We call this software Impromptica. Impromptica is a lightweight system which integrates with the existing Vamp plugin architecture for feature recognition. It uses state-of-the-art algorithms to model and accompany an input audio file.

Impromptica uses Python, SciPy, and Vamp plugins for analysis and feature extraction, and handful of other tools for music generation, including Audiolab, Hydrogen drum kits, and soundfonts. The general goal of Impromptica is to provide a simple way to expand on and explore musical ideas --- like whistled tunes or short piano ostinatos --- via accompaniment.

\subsection{Background}

Generative music and feature extraction have a rich, though young, background. Generative music describes the process of using software programs to create music using a myriad of techniques, such as generative grammars or procedural processes \cite{wooller2005framework}.

While some music generation techniques are solely functional, with no apriori information, many music generation techniques depend on musical feature recognition via feature extraction, which is still an evolving field. Feature extraction includes obtaining information about the underlying metric structure of music, such as the tempo and rhythm, and melody-related information, such as note frequencies and the locations of note onsets. The analysis of audio waveforms (in both the time and frequency domains) is used to accomplish this recognition, and novel algorithms are published frequently exploring different techniques with varying success. This information informs decisions throughout the accompaniment generation. 

Music accompaniment often occurs in a context where the score of the musical piece is known. Prior research such as \cite{dannenberg1984line} focused on matching an input audio signal to a known musical score. Our work focuses on accompaniment in an environment where the musical score is not known. Furthermore, we allow for the possibility that the input audio is original piece of music which might not ever have been represented as a musical score.

Recent research has explored concepts such as using k-NN regression for tempo recognition \cite{eronen2010music} and using spectral smoothness to analyze the frequency of polyphonic music \cite{klapuri2003multiple}. Some areas have yet to be heavily explored, such as musical analysis via machine learning.

Work has been done on automatically accompanying vocal tracks with chords \cite{simon2008mysong} using machine learning. \cite{morris2008exposing}

Feature extraction plays an important role in music generation, and forms the foundation of music accompaniment. Rather than accompany an input audio file directly, musical features are extracted from the file and used to inform decisions made throughout the course of accompaniment generation.

\subsection{Motivation}

Despite our limited background in music and music theory, we had an interest in exploring generative music using machines. Music and math are strongly connected. The intersection of computer science and music theory has produced valuable results, but many avenues are still unexplored. Investigating music theory through the lens of mathematics and computing yields opportunities to explore discrete frequencies of sound and how their complex interactions, together with rhythm, produce music. Probabilistic analysis of music offers a simple, elegant way to mathematically interpret music, model the components of a musical piece, and, we believe, generate musical accompaniment.

Compared to other methods of algorithmic music composition, many of which consist of a patchwork of hard-coded rules and heuristics, probabilistic music generation offers a higher-level approach. By learning parameters from a corpus of existing music, probabilistic music generation promises musical insights powered by Bayes' Rule:

{\small
  \[ P(\text{structure}|\text{surface}) = \frac{P(\text{surface}|\text{structure})P(\text{structure})}{P(\text{surface})} \]
}

Automatic music accompaniment has several applications. It can be used as a creativity aid in the music composition process. It can faciliate multi-instrument performances by solo performers. It can enable rapid prototyping of a new musical piece. It can be used during improvisational performances.

\subsection{Key contributions}

In building Impromptica, we made the following contributions:

\begin{enumerate}
        \item{We implemented polyphonic key-finding algorithms based on those found in \cite{temperly2007mprob}, meter recognition algorithms from \cite{klapuri2006analysis}, beat tracking algorithms from \cite{ellis2007beat}, and pitch detection algorithms from \cite{klapuri2003multiple}.}
        \item{We implemented sound manipulation and generation methods which can render musical notes as square waves or have them rendered as soundfounts via the PyFluidSynth package. We also wrote code to load and use Hydrogen drumkits.}
        \item{We integrated existing state-of-the art feature extraction algorithms into Impromptica through the Vamp plugin architecture.}
        \item{We wrote genetic algorithm code to power Impromptica's accompaniment.}
        \item{We developed software (Impromptica) which is open-source and may be useful to others.}
\end{enumerate}

\section{Methodology}

The project has feature extraction and accompaniment components.

\subsection{Software environment}

Impromptica is written in Python and currently has several dependencies, which are described in the repository README.

\subsection{Feature extraction}

The input audio file is processed through a variety of feature extraction modules which annotate the original audio track with information about musical features such as tempo and the location and duration of note onsets within the piece. Impromptica delegates the task of feature extraction to the Sonic Annotator and the Vamp plugin architecture \cite{cannam2010sonic}, a system through which state-of-the-art audio analysis algorithms can be interacted with through a uniform API. Prior to discovering Vamp, we had implemented polyphonic key-finding algorithms based on those found in Music and Probability \cite{temperly2007mprob}, meter recognition algorithms from \cite{klapuri2006analysis}, beat tracking algorithms from \cite{ellis2007beat}, and pitch detection algorithms from \cite{klapuri2003multiple}. Upon discovering Vamp, we opted to delegate feature recognition tasks to that system. We removed most of our custom implementations of feature recognition algorithms in order to keep Impromptica lean and focused on accompaniment generation.

Upon receiving the input audio file, Impromptica creates a subprocess for Sonic Annotator, a program for running Vamp plugins on audio files in batches. We maintain a set of configuration files for beat, onset, segmentation, and note plugins from the Queen Mary plugin set, which Sonic Annotator reads when preparing to run the plugins. The outputs of the plugins are consolidated into a single text file containing RDF-formatted feature information.

\subsection{Accompaniment generation}

We use genetic algorithms in combination with probabilistic generative models to generate the accompaniment. Our use of genetic algorithms stems from its applicability to problems where it is important to find a good, but not necessarily best, solution in a reasonable amount of time. Since our musical models introduce the possibility for error, and our fitness functions serve more as heuristics than as direct truth regarding musical listenability, the value of an optimal accompaniment according to our framework is not likely to exceed that of a suboptimal good accompaniments by a large amount. Genetic algorithms have been used successfully for musical improvisation in the past, which also contributed to our choice.

The probablistic generative model uses probability distributions from \cite{tempereley2007music} to generate the next note as a function of the previous note. Additionally, mutations of possible accompaniments occur probabilistically.

We generate patterns for accompaniment (e.g. note sequences) by generating variations from patterns identified in the input audio file. To do this, we use a genetic algorithm which uses mutation operations to model a population of possible accompaniments in which good accompaniments are more likely to pass on their traits to subsequent generations.

\subsubsection{Genetic algorithm framework}

We implemented a modified version of the traditional genetic algorithm; instead of selecting parents and crossing over their features into next generation individuals, new individuals are generated solely by mutations. Each individual has an array of notes being played on each tatum, a tatum being the smallest time unit on the metric scale. The initial population consists of a randomly mutated set of individuals, using the features of the input music piece as the zeroth generation parent. Five different methods of mutation are used - removal of a note, addition of a note, addition of a fifth note, a major third note and a minor third note. 

\subsubsection{Fitness}

After mutation, the fitness of each new individual is evaluated. The fitness function judges the individual on three different criteria - probabilistic consonance, note density per tatum and key uniformity. The probabilistic dissonance uses probability distribution \cite{tempereley2007music} to compare successive notes for consonance. We did not want our accompaniment to be note-heavy to the extent of overpowering the original piece. For this, the fitness of an individual is penalized if there are several accompaniment notes being played within the timeframe of a tatum. We use a naiive algorithm to extract the key of the piece and penalize if the accompaniment strays from the key of the original music piece.

\subsection{Instrumentation}

Once an accompaniment has been selected, Impromptica renders it on top of the original audio track. At present, it renders notes using a square wave generator. It is also configured to allow rendering of notes using soundfonts.

\section{Deliverables}

\subsection{Feature detection algorithms}

We implemented polyphonic key-finding algorithms based on those found in \cite{temperly2007mprob}, meter recognition algorithms from \cite{klapuri2006analysis}, beat tracking algorithms from \cite{ellis2007beat}, and pitch detection algorithms from \cite{klapuri2003multiple}.} However, as previously stated, we scrapped this code in favor of using existing Vamp plugins.

\subsection{Accompaniment generation logic}

Impromptica uses a genetic algorithm to generate and select its accompaniment. We did not implement logic to determine which regions of the piece are most appropriate for accompaniment, as we had originally planned. We also did not implement multiple-instrument accompaniment. This would have been one of our immediate next tasks, if time had remained.

\subsection{Command-line interface}

Through the command-line interface, a user may specify an input audio file for Impromptica to provide accompaniment for. Taking that file as input, Impromptica renders musical accompaniment and provides the user with the resulting audio file.

\subsection{Web interface}

We did not implement a web interface for Impromptica, which we had originally planned to do.

\subsection{Experimentation, data collection and visualization}

We will analyze the efficacy of probabilistic music generation by comparing the music it creates to that generated by alternative methods.  We will also compare the overall aesthetic of the generated music to that of conventional music.

\section{Discussion}

\subsection{Music theory}

As a precursor to any implementation, we had to spend some time understanding some fundamental music concepts and how music has been modeled in different systems throughout history. We supplemented this exploration through several books, such as \cite{temperly2007mprob}, \cite{krogerGeeksNerds}, and \cite{loy2006musimathics}. This provided several insights into the mathematical nature of music that we were able to map to a computational realm. For instance, the equal-tempered chromatic scale functions on a logarithmic scale - that is, a particular semitones's frequency can be modeled by the equation
{\small
	\[frequency = 261.63 * 2^{(\text{value} - 60) / 12.0)}\]
}
where 261.63 represents the frequency of middle C, 60 represents the semitone of middle C, and value represents the semitone we are solving for, relative to middle C.

Other insights included a exploration of timbre and soundfonts, permutation techniques like inversion and retrograde, interval sonorities, and several other musical constructs.

\subsection{Music ontology for feature extraction}

A large body of work is forming in the area of musical feature extraction. Much of this work is fueled by interests in music information retrieval, where algorithms run on music datasets to identify metadata for applications such as recommender systems. As the number of extracted features has grown, some focus has shifted towards the development of feature extraction frameworks and standards. In such systems, unrelated feature extraction tools may plug in to existing feature extraction systems with little or no additional configuration. The Vamp plugins system takes this approach, defining an API through which programmers may write feature extraction plugins. Tools such as Sonic Annotator provide a mechanism to apply specified Vamp plugins to specified audio files.

Prior work has examined how feature extraction can be represented through a modular, unifying ontology. \cite{raimond2008web}. Vamp plugins use (non-excusively) the music \cite{raimond2007music} and event \cite{raimond2007event} ontologies to represent extracted features. The SAWA web application \cite{fazekas2009reusable} uses many of the previously-mentioned tools to provide an online API for audio analysis.

One downside of the RDF approach is that the data is not as highly compressed as it could be. The data is not stored in a binary format, and there is a good deal of duplication. This also creates costs of parsing the data.

The RDF representation's benefit is that it offers a single interface through which to retrieve various data of different types.

Running multiple feature extraction tools on an audio file presents the opportunity to eliminate redudant transformations of the audio. For example, many feature extraction tools utilize information from the frequency domain, obtained by taking windowed Fast Fourier Transform at steps throughout the piece. Such a derived signal could be calculated once and reused by each feature extraction tool which needs it. YAAFE \cite{mathieu2010yaafe} is an example of a system which attempts to remove redundant steps in the plans it creates to extract specified features from input audio files.

\subsection{Genetic algorithms for music composition}

Genetic algorithms use mutation (such as flipping a bit in the bitstring representation of an individual) or crossover (where new individuals take on attributes from multiple parents in the previous generation.) GenJam used mutation. Rather than flipping bits, it attempted to make musically-meaningful mutations.

\subsection{Fitness functions and heuristics}

We lack a purely programmatic mechanism to accurately to assess the aesthetic beauty of a piece of music, whether received or generated.

Fitness functions have been used in genetic programming implementations of music composition. \cite{towsey2001towards} lists several features which can be mutated in such a process.

In GenJam \cite{biles1994genjam}, human feedback was used to determine which of two jazz solos created through genetic programming were better, but the narrow bandwidth of such feedback was noted. An unsuccessful attempt was made in \cite{biles1996neural} to augment the human feedback with a trained neural network. However, the system went on to be used in hundreds of live performances, and was calibrated to the point that it was a successful component of the performances. A thorough treatment is given in \cite{biles2007improvizing}. Impromptica follows much in the spirit of GenJam.

Algorithmic music composition via genetic programming was used to some success in Vox Populi \cite{moroni2000vox}.

Current research continues to explore and refine fitness functions for music. Machine learning lends itself to the configuration of fitness functions. Recent research has explored using hierarchical self-organizing maps to predict musical patterns \cite{law2008towards}. Such models exploit the idea of human expectation as a factor of musical experience. A musical sequence predictor with sufficient power can be used as a component in a fitness function to assess how expected a given musical sequence may be by the listener.

In \cite{horner1991genetic}, genetic algorithms were used to generate bridges between two note patterns.

\subsection{Relation to music composition}

The field of music composition is more mature than that of accompaniment. Accompaniment (to an unknown score) could be seen as a subset of composition involving mediation between multiple parties.

\subsection{Machine learning for accompaniment}

The MySong project, which used machine learning to select chords for accompaniment of a vocal melody, exposed some model parameters to end users \cite{morris2008exposing}. To keep things simple, Impromptica does not take this approach, choosing instead to make educated guesses of appropriate parameter values given the data it already had.

\subsection{Real-time accompaniment challenges}

Impromptica, in its current form, does not attempt to accompany music in real time. Real-time accompaniment is more difficult because future features are not known and must be predicted from past and present extracted features.

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{report}

\end{document}
